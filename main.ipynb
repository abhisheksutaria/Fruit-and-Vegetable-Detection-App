{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os, os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Training Samples: 2980\n",
      "No. of Test Samples: 2003\n"
     ]
    }
   ],
   "source": [
    "train_categories = []\n",
    "train_samples = []\n",
    "for i in os.listdir(\"./data/merged/train\"):\n",
    "    train_categories.append(i)\n",
    "    train_samples.append(len(os.listdir(\"./data/merged/train/\" + i)))\n",
    "\n",
    "test_categories = []\n",
    "test_samples = []\n",
    "for i in os.listdir(\"./data/merged/test\"):\n",
    "    test_categories.append(i)\n",
    "    test_samples.append(len(os.listdir(\"./data/merged/test/\" + i)))\n",
    "\n",
    "print(\"No. of Training Samples:\", sum(train_samples))\n",
    "print(\"No. of Test Samples:\", sum(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "# fig_size[0] = 30\n",
    "# fig_size[1] = 5\n",
    "# plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "#\n",
    "# index = np.arange(len(train_categories))\n",
    "# plt.bar(index, train_samples)\n",
    "# plt.xlabel('Categories', fontsize=25)\n",
    "# plt.ylabel('No. of samples', fontsize=25)\n",
    "# plt.xticks(index, train_categories, fontsize=15, rotation=90)\n",
    "# plt.title('Category wise training sample distribution', fontsize=35)\n",
    "# plt.show()\n",
    "\n",
    "# index2 = np.arange(len(test_categories))\n",
    "# plt.bar(index2, test_samples)\n",
    "# plt.xlabel('Categories', fontsize=25)\n",
    "# plt.ylabel('No. of samples', fontsize=25)\n",
    "# plt.xticks(index2, test_categories, fontsize=15, rotation=90)\n",
    "# plt.title('Category wise test sample distribution', fontsize=35)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in os.listdir(\"./data/merged/train\"):\n",
    "    one_hot = np.zeros(shape=[len(train_categories)])\n",
    "    actual_index = train_categories.index(i)\n",
    "    one_hot[actual_index] = 1\n",
    "    for files in os.listdir(\"./data/merged/train/\" + i):\n",
    "        img_array = mpimg.imread(\"./data/merged/train/\" + i + \"/\" + files)\n",
    "        train.append([img_array, one_hot])\n",
    "    # print(\"Train Category Status: {}/{}\".format(actual_index+1, len(train_categories)))\n",
    "\n",
    "for i in os.listdir(\"./data/merged/test\"):\n",
    "    one_hot = np.zeros(shape=[len(test_categories)])\n",
    "    actual_index = test_categories.index(i)\n",
    "    one_hot[actual_index] = 1\n",
    "    for files in os.listdir(\"./data/merged/test/\" + i):\n",
    "        img_array = mpimg.imread(\"./data/merged/test/\" + i + \"/\" + files)\n",
    "        test.append([img_array, one_hot])\n",
    "    # print(\"Test Category Status: {}/{}\".format(actual_index+1, len(test_categories)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(len(train)):\n",
    "    train_x.append(train[i][0])\n",
    "    train_y.append(train[i][1])\n",
    "    #print(\"Status {}/{}\".format(i+1, len(train)))\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(test)):\n",
    "    test_x.append(test[i][0])\n",
    "    test_y.append(test[i][1])\n",
    "    #print(\"Status {}/{}\".format(i+1, len(test)))\n",
    "\n",
    "idx = np.random.choice(len(train), size=math.floor(len(train)*0.2))\n",
    "validation_from_test_x = []\n",
    "validation_from_test_y = []\n",
    "\n",
    "for i in range(len(idx)):\n",
    "    validation_from_test_x.append(test[i][0])\n",
    "    validation_from_test_y.append(test[i][1])\n",
    "    #print(\"Status {}/{}\".format(i+1, len(idx)))\n",
    "\n",
    "\n",
    "training_x = np.asarray(train_x, dtype=np.float32)\n",
    "training_y = np.asarray(train_y, dtype=np.float32)\n",
    "testing_x = np.asarray(test_x, dtype=np.float32)\n",
    "testing_y = np.asarray(test_y, dtype=np.float32)\n",
    "validation_x = np.asarray(validation_from_test_x, dtype=np.float32)\n",
    "validation_y = np.asarray(validation_from_test_y, dtype=np.float32)\n",
    "\n",
    "for i in range(len(training_x)):\n",
    "    training_x[i] = training_x[i]/255\n",
    "\n",
    "for i in range(len(testing_x)):\n",
    "    testing_x[i] = testing_x[i]/255\n",
    "\n",
    "for i in range(len(validation_x)):\n",
    "    validation_x[i] = validation_x[i]/255\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 100, 100, 16)      2352      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 100, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        12800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        50176     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 44, 44, 64)        51200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 128)       204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 2,528,132\n",
      "Trainable params: 2,525,028\n",
      "Non-trainable params: 3,104\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "94/94 [==============================] - 119s 1s/step - loss: 1.0666 - accuracy: 0.6813 - val_loss: 7.8672 - val_accuracy: 0.2013\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 115s 1s/step - loss: 0.3958 - accuracy: 0.8763 - val_loss: 5.6825 - val_accuracy: 0.3305\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 122s 1s/step - loss: 0.2487 - accuracy: 0.9257 - val_loss: 3.3866 - val_accuracy: 0.5017\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 123s 1s/step - loss: 0.2390 - accuracy: 0.9294 - val_loss: 1.6018 - val_accuracy: 0.5621\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 118s 1s/step - loss: 0.2033 - accuracy: 0.9395 - val_loss: 5.4085 - val_accuracy: 0.3238\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 122s 1s/step - loss: 0.1352 - accuracy: 0.9608 - val_loss: 0.7897 - val_accuracy: 0.6930\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 119s 1s/step - loss: 0.1335 - accuracy: 0.9586 - val_loss: 0.1959 - val_accuracy: 0.9497\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 119s 1s/step - loss: 0.0519 - accuracy: 0.9866 - val_loss: 0.1432 - val_accuracy: 0.9480\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 120s 1s/step - loss: 0.1085 - accuracy: 0.9689 - val_loss: 0.3020 - val_accuracy: 0.9027\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 123s 1s/step - loss: 0.0694 - accuracy: 0.9825 - val_loss: 0.0373 - val_accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# glorot_normal means xavier normal initializer\n",
    "model.add(Conv2D(input_shape=(100, 100, 3), kernel_size=(7,7), filters=16, padding='same', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(kernel_size=(5,5), filters=32, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(strides=2, padding='valid'))\n",
    "\n",
    "model.add(Conv2D(kernel_size=(7,7), filters=32, padding='same', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(kernel_size=(5,5), filters=64, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(strides=2, padding='valid'))\n",
    "\n",
    "model.add(Conv2D(kernel_size=(5,5), filters=128, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(kernel_size=(3,3), filters=256, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(strides=2, padding='valid'))\n",
    "\n",
    "model.add(Conv2D(kernel_size=(3,3), filters=256, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(kernel_size=(3,3), filters=512, padding='valid', use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(256, use_bias=False, activation=None, kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(len(train_categories), activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(\"Fruits_recognition\"))\n",
    "# log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model.fit(x=training_x, y=training_y, batch_size=32, epochs=10,  callbacks=[tensorboard], validation_data=(validation_x, validation_y))\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 15s 236ms/step\n",
      "Test Accuracy:  94.10883674488268\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "predict_y = model.predict(x=testing_x, batch_size=32, verbose=1)\n",
    "for i in range(len(testing_x)):\n",
    "    if (np.argmax(predict_y[i]) == np.argmax(testing_y[i])):\n",
    "        test_accuracy += 1\n",
    "\n",
    "test_accuracy = test_accuracy / len(testing_x)*100\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
